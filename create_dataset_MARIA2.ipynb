{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本部分代码用于处理完整的数据集的数据以提取出符合要求的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import random\n",
    "# 初始化随机种子保证可复现性\n",
    "random_seed = 43\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = r'./your_path/mimic-cxr-2.0.0-split.csv'\n",
    "meta = r'./your_path/mimic-cxr-2.0.0-metadata.csv'\n",
    "txt = r'./your_path/secction_csv/mimic_cxr_all_sectioned.csv'\n",
    "# 加载 CSV 文件\n",
    "split_df = pd.read_csv(split_data)  # 包含 dicom_id, study_id, subject_id, split\n",
    "sectioned_df = pd.read_csv(txt)  # 包含 study, findings, indication\n",
    "metadata_df = pd.read_csv(meta)  # 包含 dicom_id, subject_id, study_id, ViewPosition\n",
    "sectioned_df['study'] = sectioned_df['study'].str[1:].astype(int)  # 去掉前面的 's'\n",
    "# sectioned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并metadata和sectioned数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(metadata_df, sectioned_df[['study', 'impression','findings', 'indication', 'technique','comparison' ]], how='left', left_on='study_id', right_on='study')\n",
    "# merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并split数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(merged_df, split_df[['dicom_id', 'study_id', 'subject_id', 'split']], on=['dicom_id', 'study_id', 'subject_id'], how='left')\n",
    "# final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数据进行进一步的处理，排序，格式变换，选取图像等操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按subject_id分组，按时间排序study\n",
    "final_df = final_df.sort_values([\"subject_id\", \"StudyDate\",\"StudyTime\"])\n",
    "# 标记每个study的前一个study（Prior）\n",
    "final_df[\"prior_study_id\"] = final_df.groupby(\"subject_id\")[\"study_id\"].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df[final_df[\"subject_id\"] == 10002428]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[['dicom_id', 'subject_id', 'study_id', \"StudyDate\", 'StudyTime', 'split','ViewPosition','findings','indication','comparison','technique','prior_study_id', 'ViewCodeSequence_CodeMeaning']]\n",
    "# 构建图片的 path 列\n",
    "final_df['dicom_id'] = final_df.apply(\n",
    "    lambda row: os.path.join(f\"p{str(row['subject_id'])[:2]}\", f\"p{row['subject_id']}\", f\"s{row['study_id']}\", f\"{row['dicom_id']}.jpg\"),\n",
    "    axis=1\n",
    ")\n",
    " # 使用正则表达式，将那些只由下划线和点组成的字符串替换成 np.nan\n",
    "final_df = final_df.replace({None: np.nan})\n",
    "final_df = final_df.replace(r'^[_., ]+$', np.nan, regex=True)\n",
    "final_final_df = final_df.replace(r'^[_.\\s]*(None)[_.\\s]*$', np.nan, regex=True)        #这部分其实没有用到，也就是说None等没有替换\n",
    "# final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义正面和侧面的取值范围\n",
    "frontal_codes = {\"antero-posterior\", \"postero-anterior\"}\n",
    "frontal_positions = {\"AP\", \"PA\", \"AP AXIAL\", \"PA LLD\", \"PA RLD\",\"AP LLD\"}\n",
    "\n",
    "lateral_codes = {\"lateral\", \"left lateral\"}\n",
    "lateral_positions = {\"LATERAL\", \"LL\", \"XTABLE LATERAL\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_study(group):\n",
    "    \"\"\"处理单个study，生成候选数据行\"\"\"\n",
    "    current_study_id = group.name\n",
    "    subject_id = group[\"subject_id\"].iloc[0]\n",
    "    \n",
    "    # 提取当前study的Frontal/Lateral影像\n",
    "    current_frontal = group[(group[\"ViewCodeSequence_CodeMeaning\"].isin(frontal_codes)) | (group[\"ViewPosition\"].isin(frontal_positions))][\"dicom_id\"].tolist()\n",
    "    current_lateral = group[(group[\"ViewCodeSequence_CodeMeaning\"].isin(lateral_codes)) | (group[\"ViewPosition\"].isin(lateral_positions))][\"dicom_id\"].tolist()\n",
    "    \n",
    "    # 必须存在Frontal影像\n",
    "    if not current_frontal:\n",
    "        return pd.DataFrame()  # 跳过无效数据\n",
    "    # 为每个frontal随机分配一个lateral\n",
    "    lateral_mapping = {\n",
    "        frontal: random.choice(current_lateral) if current_lateral else None\n",
    "        for frontal in current_frontal\n",
    "    }\n",
    "    # print(\"__________________________________\")\n",
    "    # print(group.columns)\n",
    "    # 提取Prior Study信息\n",
    "    prior_study_id = group[\"prior_study_id\"].iloc[0]\n",
    "    prior_data = {\n",
    "        \"frontal\": [],\n",
    "        \"reports\": []\n",
    "    }\n",
    "    # print(\"__________________________________\")\n",
    "    if pd.notna(prior_study_id):\n",
    "        prior_study = final_df[final_df[\"study_id\"] == prior_study_id]\n",
    "        if not prior_study.empty:\n",
    "            # 提取所有prior frontal影像\n",
    "            prior_frontal = prior_study[(prior_study[\"ViewCodeSequence_CodeMeaning\"].isin(frontal_codes)) | (prior_study[\"ViewPosition\"].isin(frontal_positions))][\"dicom_id\"].tolist()\n",
    "            # 提取报告信息并组合\n",
    "            report_components = []\n",
    "            for field in [\"indication\", \"technique\", \"comparison\", \"findings\"]:\n",
    "                value = prior_study[field].iloc[0] + \" \"\n",
    "                if pd.notna(value):\n",
    "                    report_components.append(f\"{field.upper()}: {value}\")\n",
    "            prior_report = \"\\n\\n\".join(report_components) if report_components else None\n",
    "            if prior_report:\n",
    "                print(\"报告非空\")\n",
    "            else:\n",
    "                print(\"报告为空\")\n",
    "            # 建立映射关系\n",
    "            prior_data[\"frontal\"] = prior_frontal\n",
    "            prior_data[\"reports\"] = [prior_report] * len(prior_frontal)\n",
    "        # else:\n",
    "        #     prior_frontal, prior_report = [], None\n",
    "    # ======================== 生成数据组合 ========================\n",
    "    rows = []\n",
    "    for frontal in current_frontal:\n",
    "        # 当前影像组合\n",
    "        current_pair = {\n",
    "            \"Current_frontal_dicom_id\": frontal,\n",
    "            \"Current_lateral_dicom_id\": lateral_mapping[frontal]\n",
    "        }\n",
    "        # 先前影像处理\n",
    "        if prior_data[\"frontal\"]:\n",
    "            for prior_frontal, prior_report in zip(prior_data[\"frontal\"], prior_data[\"reports\"]):\n",
    "                rows.append({\n",
    "                    **current_pair,\n",
    "                    \"Prior_frontal_dicom_id\": prior_frontal,\n",
    "                    \"subject_id\": subject_id,\n",
    "                    \"study_id\": current_study_id,\n",
    "                    \"prior_report\": prior_report,\n",
    "                    \"findings\": group[\"findings\"].iloc[0],\n",
    "                    \"indication\": group[\"indication\"].iloc[0],\n",
    "                    \"comparison\": group[\"comparison\"].iloc[0],\n",
    "                    \"technique\": group[\"technique\"].iloc[0],\n",
    "                    \"split\": group[\"split\"].iloc[0]\n",
    "                })\n",
    "        else:\n",
    "            rows.append({\n",
    "                **current_pair,\n",
    "                \"Prior_frontal_dicom_id\": None,\n",
    "                \"subject_id\": subject_id,\n",
    "                \"study_id\": current_study_id,\n",
    "                \"prior_report\": None,\n",
    "                \"findings\": group[\"findings\"].iloc[0],\n",
    "                \"indication\": group[\"indication\"].iloc[0],\n",
    "                \"comparison\": group[\"comparison\"].iloc[0],\n",
    "                \"technique\": group[\"technique\"].iloc[0],\n",
    "                \"split\": group[\"split\"].iloc[0]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# 按study分组处理\n",
    "final_df = final_df.groupby(\"study_id\",sort=False, group_keys=False).apply(process_study)\n",
    "final_df = final_df[\n",
    "    final_df[\"findings\"].notna() & \n",
    "    final_df[\"Current_frontal_dicom_id\"].notna()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改 'report' 列的数据\n",
    "final_df['findings'] = final_df['findings'].str.replace('\\n', '').replace('\\r', '')\n",
    "final_df['findings'] = final_df['findings'].str.replace(\"  \", \" \")\n",
    "final_df['indication'] = final_df['indication'].str.replace('\\n', '').replace('\\r', '')\n",
    "final_df['indication'] = final_df['indication'].str.replace(\"  \", \" \")\n",
    "final_df['prior_report'] = final_df[\"prior_report\"].str.replace('\\n', '').replace('\\r', '')\n",
    "final_df['prior_report'] = final_df[\"prior_report\"].str.replace(\"  \", \" \")\n",
    "final_df['comparison'] = final_df[\"comparison\"].str.replace('\\n', '').replace('\\r', '')\n",
    "final_df['comparison'] = final_df[\"comparison\"].str.replace(\"  \", \" \")\n",
    "final_df['technique'] = final_df[\"technique\"].str.replace('\\n', '').replace('\\r', '')\n",
    "final_df['technique'] = final_df[\"technique\"].str.replace(\"  \", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(final_df))\n",
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保证Prior_report以及prior_frontal有效"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将时间信息重新添加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(metadata_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df['dicom_id'] = metadata_df.apply(\n",
    "    lambda row: os.path.join(f\"p{str(row['subject_id'])[:2]}\", f\"p{row['subject_id']}\", f\"s{row['study_id']}\", f\"{row['dicom_id']}.jpg\"),\n",
    "    axis=1\n",
    ")\n",
    "# metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, metadata_df[['StudyDate', 'StudyTime','dicom_id']], how='left', left_on='Current_frontal_dicom_id', right_on='dicom_id')\n",
    "final_df.rename(columns={'StudyDate': 'C_Date', 'StudyTime': 'C_Time'}, inplace=True)\n",
    "final_df = final_df[['Current_frontal_dicom_id','Current_lateral_dicom_id','Prior_frontal_dicom_id', 'subject_id', 'study_id', 'prior_report' ,'findings','indication','comparison','technique','C_Date','C_Time','split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()\n",
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, metadata_df[['StudyDate', 'StudyTime','dicom_id']], how='left', left_on='Prior_frontal_dicom_id', right_on='dicom_id')\n",
    "final_df.rename(columns={'StudyDate': 'P_Date', 'StudyTime': 'P_Time'}, inplace=True)\n",
    "final_df = final_df[['Current_frontal_dicom_id','Current_lateral_dicom_id','Prior_frontal_dicom_id', 'subject_id', 'study_id', 'prior_report' ,'findings','indication','comparison','technique','C_Date','C_Time','P_Date','P_Time','split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['study_id'] = final_df['study_id'].apply(lambda x: 's' + str(x))  # 这里添加了3个\"s\"\n",
    "\n",
    "# 按 split 列进行分组：训练集、验证集、测试集\n",
    "train_df = final_df[final_df['split'] == 'train']\n",
    "val_df = final_df[final_df['split'] == 'validate']\n",
    "\n",
    "\n",
    "# 输出数据框的前几行\n",
    "# print(train_df.head())\n",
    "# print(test_df.head())\n",
    "# print(val_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存为 CSV 文件\n",
    "train_df.to_csv('./your_path/train_data.csv', index=False)\n",
    "val_df.to_csv('./your_path/val_data.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"Data processing complete. The datasets have been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检验时间顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(val_df)\n",
    "df = pd.read_csv('./datasets/MIMIC-complete/processed_data_MARIA1/test_data.csv', encoding='utf-8')\n",
    "df[\"study_id\"].nunique()\n",
    "styD = list(final_df[\"StudyDate\"].unique())\n",
    "styT = list(final_df[\"StudyTime\"].unique())\n",
    "# for i in range(final_df[\"StudyDate\"].nunique()):\n",
    "#     print(f\"studyDate:{styD[i]},studyTime:{styT[i]}\")\n",
    "metadata_df[\"StudyTime\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_time_counts = metadata_df.groupby(['subject_id','study_id','StudyDate'])['StudyTime'].nunique()\n",
    "inconsistent_dates = study_time_counts[study_time_counts > 1].index\n",
    "inconsistent_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设您的 DataFrame 名为 metadata_df，包含 'subject_id'、'StudyDate' 和 'StudyTime' 列\n",
    "grouped = metadata_df.groupby(['subject_id', 'StudyDate'])['StudyTime'].nunique().reset_index()\n",
    "\n",
    "# 筛选出 'StudyTime' 不唯一的组合\n",
    "inconsistent = grouped[grouped['StudyTime'] > 1]\n",
    "\n",
    "# 合并原始数据以查看具体记录\n",
    "inconsistent_records = pd.merge(metadata_df, inconsistent[['subject_id', 'StudyDate']], on=['subject_id', 'StudyDate'], how='inner')\n",
    "\n",
    "print(inconsistent_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_time_counts = metadata_df.groupby(['subject_id','StudyDate'])['StudyTime'].nunique()\n",
    "inconsistent_dates = study_time_counts[study_time_counts > 1].index\n",
    "inconsistent_samples = metadata_df[metadata_df['StudyDate'].isin(inconsistent_dates)]\n",
    "for date in inconsistent_dates:\n",
    "    times = metadata_df[metadata_df['StudyDate'] == date]['StudyTime'].unique()\n",
    "    print(f\"StudyDate: {date}, Unique StudyTimes: {times}\")\n",
    "\n",
    "\n",
    "# # 假设您的 DataFrame 名为 metadata_df，包含 'subject_id'、'StudyDate' 和 'StudyTime' 列\n",
    "# grouped = metadata_df.groupby(['subject_id', 'StudyDate'])['StudyTime'].nunique().reset_index()\n",
    "# inconsistent = grouped[grouped['StudyTime'] > 1]\n",
    "# print(inconsistent)\n",
    "# metadata_df['time_count'] = metadata_df.groupby(['subject_id', 'StudyDate'])['StudyTime'].transform('nunique')\n",
    "\n",
    "# # 筛选出 StudyTime 数量大于1的记录\n",
    "# inconsistent_records = metadata_df[metadata_df['time_count'] > 1]\n",
    "\n",
    "# # 删除辅助列\n",
    "# inconsistent_records = inconsistent_records.drop(columns='time_count')\n",
    "# inconsistent_records = inconsistent_records[['subject_id',  'study_id', 'StudyDate','StudyTime']]\n",
    "# # print(f\"{inconsistent_records[\"subject_id\"]},{inconsistent_records[\"StudyDate\"]},{inconsistent_records[\"StudyTime\"]}\")\n",
    "\n",
    "# print(inconsistent_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 抽取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import final\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./your_path/train_data.csv', encoding='utf-8')\n",
    "\n",
    "class1 = df[(df['Prior_frontal_dicom_id'].isnull()) & (df['Current_lateral_dicom_id'].isnull())]    # (1)\n",
    "class2 = df[(df['Prior_frontal_dicom_id'].notnull()) & (df['Current_lateral_dicom_id'].isnull())]  # (2)\n",
    "class3 = df[(df['Prior_frontal_dicom_id'].isnull()) & (df['Current_lateral_dicom_id'].notnull())]  # (3)\n",
    "class4 = df[(df['Prior_frontal_dicom_id'].notnull()) & (df['Current_lateral_dicom_id'].notnull())] # (4)\n",
    "\n",
    "# print(len(class1))\n",
    "# 计算各类数据占总数据的比例\n",
    "total = len(df)\n",
    "prop1 = len(class1) / total\n",
    "prop2 = len(class2) / total\n",
    "prop3 = len(class3) / total\n",
    "prop4 = len(class4) / total\n",
    "\n",
    "\n",
    "\n",
    "print(f\"数据总量:{total}\")\n",
    "print(\"类别 1 (Prior为空, Current为空): 数据量为{}占比为{:.2%}\".format(len(class1),prop1))\n",
    "print(\"类别 2 (Prior非空, Current为空): 数据量为{}占比为{:.2%}\".format(len(class2),prop2))\n",
    "print(\"类别 3 (Prior为空, Current非空): 数据量为{}占比为{:.2%}\".format(len(class3),prop3))\n",
    "print(\"类别 4 (Prior非空, Current非空): 数据量为{}占比为{:.2%}\".format(len(class4),prop4))\n",
    "\n",
    "# 设定新数据集的总样本数，例如 new_total = 1000\n",
    "new_total = 30000\n",
    "\n",
    "# 计算每类需要抽样的样本数（这里使用 int 直接取整，也可以用 round 处理四舍五入）\n",
    "n_class1 = int(prop1 * new_total)\n",
    "n_class2 = int(prop2 * new_total)\n",
    "n_class3 = int(prop3 * new_total)\n",
    "n_class4 = int(prop4 * new_total)\n",
    "print(n_class1,n_class2,n_class3,n_class4,sep='\\n')\n",
    "# 根据各类比例从原始数据中抽样\n",
    "new_class1 = class1.sample(n=n_class1, random_state=random_seed)\n",
    "new_class2 = class2.sample(n=n_class2, random_state=random_seed)\n",
    "new_class3 = class3.sample(n=n_class3, random_state=random_seed)\n",
    "new_class4 = class4.sample(n=n_class4, random_state=random_seed)\n",
    "\n",
    "# 合并抽样结果，构成新的数据集（可以打乱顺序）\n",
    "new_dataset = pd.concat([new_class1, new_class2, new_class3, new_class4]).sample(frac=1, random_state=random_seed)\n",
    "new_dataset = new_dataset.drop_duplicates(subset='study_id')\n",
    "final_dataset = pd.concat([df1, new_dataset], ignore_index=True)\n",
    "print(\"新数据集样本数:\", len(final_dataset))\n",
    "final_dataset.to_csv('./your_path', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去除除了prior_report中除了Findings的其余部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_findings_from_prior_report(prior_report_text):\n",
    "    \"\"\"\n",
    "    从完整的prior_report文本中提取'FINDINGS:'部分后的内容。\n",
    "    如果'FINDINGS:'不存在，或者其后内容为空，则返回空字符串。\n",
    "    \n",
    "    Args:\n",
    "        prior_report_text (str): 原始的prior_report文本。\n",
    "        \n",
    "    Returns:\n",
    "        str: 提取出的Findings部分内容，或空字符串。\n",
    "    \"\"\"\n",
    "    if not isinstance(prior_report_text, str):\n",
    "        return \"\" # 处理非字符串类型，如NaN，直接返回空字符串\n",
    "\n",
    "    # 正则表达式解释：\n",
    "    # r'FINDINGS:\\s*'      - 匹配字面字符串 \"FINDINGS:\" 后跟任意数量的空白字符 (包括换行符)\n",
    "    # (.*?)                 - 这是一个非贪婪捕获组，匹配任何字符 (除了换行符，如果使用re.DOTALL则包括换行符)\n",
    "    #                       - `*?` 是非贪婪的，它会尽可能少地匹配，直到遇到下一个条件\n",
    "    # (?=\\b[A-Z_]+:|\\Z)     - 这是一个正向先行断言 (Positive Lookahead)\n",
    "    #   \\b[A-Z_]+:          - 匹配一个单词边界 `\\b` 后跟一个或多个大写字母或下划线 `[A-Z_]+`\n",
    "    #                       - 再跟一个冒号 `:`。这用于匹配下一个报告节的标题 (如 TECHNIQUE:, COMPARISON:, IMPRESSION: 等)\n",
    "    #   |                   - 或者\n",
    "    #   \\Z                  - 匹配字符串的结束。\n",
    "    # re.DOTALL             - 使 '.' 匹配包括换行符在内的所有字符，确保能捕获跨行的Findings内容。\n",
    "\n",
    "    # MIMIC-CXR 报告的常见结构，通常 FINDINGS 后直到 IMPRESSION, TECHNIQUE, COMPARISON 等下一个大写标题结束\n",
    "    # 或者直到字符串结束\n",
    "    match = re.search(r'FINDINGS:\\s*(.*?)(?=\\b[A-Z_]+:|\\Z)', prior_report_text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        extracted_findings = match.group(1).strip() # .strip() 移除前后空白\n",
    "        # 根据你之前提供的样本，可能会有一些___的占位符，可以进行清洗\n",
    "        extracted_findings = extracted_findings.replace('___', '').strip() # 移除或替换占位符\n",
    "        return extracted_findings\n",
    "    else:\n",
    "        # 如果没有找到 \"FINDINGS:\" 部分，或者匹配结果为空（例如：FINDINGS: 后无内容）\n",
    "        return \"\"\n",
    "\n",
    "# --- 如何在你的数据加载和处理流程中应用 ---\n",
    "\n",
    "# 1. 假设你已经加载了 CSV 文件到 DataFrame (通常在你的主脚本开始时)\n",
    "# 示例：\n",
    "# df_train = pd.read_csv('train_data_5000_avg_43.csv')\n",
    "\n",
    "# 2. 在你使用 Hugging Face `datasets` 库处理数据之前，对DataFrame进行预处理\n",
    "# 这是在 `Hugging Face Dataset` 对象被创建或在你传递给 `map` 函数之前完成\n",
    "# 因为 map 函数通常期望接收原始列名\n",
    "#\n",
    "# 注意：你需要将这一步放在你的数据加载流程中，例如：\n",
    "# df_data = pd.read_csv('your_train_data.csv')\n",
    "# df_data['prior_report'] = df_data['prior_report'].apply(extract_findings_from_prior_report)\n",
    "# 然后再从 df_data 创建 Hugging Face Dataset 对象\n",
    "\n",
    "# --- 演示如何使用此函数 ---\n",
    "# 模拟一些 prior_report 文本\n",
    "sample_prior_reports = [\n",
    "    \"INDICATION: Shortness of breath.TECHNIQUE: X-ray.COMPARISON: None.FINDINGS: Clear lungs. No effusions. Heart size normal.\",\n",
    "    \"INDICATION: Chest pain.FINDINGS: Small right pleural effusion.IMPRESSION: Effusion noted.\",\n",
    "    \"No Findings section here. Just some random text.\",\n",
    "    \"FINDINGS: Patient is stable. No acute findings.\",\n",
    "    \"INDICATION: Fever.FINDINGS: \", # FINDINGS 后没有内容\n",
    "    None, # None值\n",
    "    \"Just findings at the start: This is the findings content.\",\n",
    "    \"FINDINGS: A nodule in the left upper lobe. TECHNIQUE: CT chest.\" # 后面跟着另一个标题\n",
    "]\n",
    "\n",
    "# 创建一个临时的DataFrame来演示\n",
    "# df_temp = pd.DataFrame({'prior_report': sample_prior_reports})\n",
    "\n",
    "# print(\"--- 原始 prior_report ---\")\n",
    "# for i, text in enumerate(df_temp['prior_report']):\n",
    "#     print(f\"Sample {i}: {text}\")\n",
    "# print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# # 应用函数来修改 prior_report 列\n",
    "# df_temp['prior_report_processed'] = df_temp['prior_report'].apply(extract_findings_from_prior_report)\n",
    "\n",
    "# print(\"--- 处理后的 prior_report (仅 Findings 部分) ---\")\n",
    "# for i, text in enumerate(df_temp['prior_report_processed']):\n",
    "#     print(f\"Sample {i}: {text if text else '[EMPTY]'}\") # 打印空字符串时显示[EMPTY]方便查看\n",
    "\n",
    "# print(\"\\n--- 原始与处理后的对照 ---\")\n",
    "# print(df_temp[['prior_report', 'prior_report_processed']].to_string())\n",
    "\n",
    "csv_file_path = \"./datasets/MIMIC-complete/processed_data_MARIA2_new/train_data_60000_avg_43.csv\" # 确保路径正确\n",
    "df_data = pd.read_csv(csv_file_path)\n",
    "print(df_data[\"prior_report\"].head(10))\n",
    "df_data['cleaned_prior_report'] = df_data['prior_report'].apply(extract_findings_from_prior_report)\n",
    "print(df_data[\"cleaned_prior_report\"].head(10))\n",
    "df_data.to_csv(\"./datasets/MIMIC-complete/processed_data_MARIA2_new/train_data_60000_avg_43.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   加入type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"./your_path\" # 确保路径正确\n",
    "df_data = pd.read_csv(csv_file_path)\n",
    "def determine_type(row):\n",
    "        # print(row.get(\"Current_lateral_dicom_id\"))\n",
    "        has_cl = pd.notna(row.get(\"Current_lateral_dicom_id\"))\n",
    "        # print(has_cl)\n",
    "        has_pf = pd.notna(row.get('Prior_frontal_dicom_id'))\n",
    "        if has_cl and has_pf:\n",
    "            return 'all'\n",
    "        elif has_cl:\n",
    "            return 'CFCL'\n",
    "        elif has_pf:\n",
    "            return 'CFPF'\n",
    "        else:\n",
    "            return 'CF'\n",
    "df_data['type'] = df_data.apply(determine_type, axis=1)\n",
    "df_data.to_csv(\"./your_Path\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分Rare和Common标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def build_subgroups(csv_path, output_path, rare_q=0.25, common_q=0.75):\n",
    "    \"\"\"\n",
    "    根据 CheXbert 标签划分 Rare / Common 亚组\n",
    "    \n",
    "    参数:\n",
    "        csv_path: 输入 CSV 文件路径\n",
    "        output_path: 输出带有 subgroup 的 CSV 文件路径\n",
    "        rare_q: 罕见标签分位数阈值 (默认 25%)\n",
    "        common_q: 常见标签分位数阈值 (默认 75%)\n",
    "    \"\"\"\n",
    "    # 1. 读取数据\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.drop_duplicates(subset='Report Impression')\n",
    "    print(len(df))\n",
    "    # 假设第一列是文本，后面14列是CheXbert标签\n",
    "    df.fillna(0, inplace=True)\n",
    "    label_cols = df.columns[1:15]\n",
    "    \n",
    "    # 2. 计算每个标签的阳性比例\n",
    "    prevalence = df[label_cols].apply(lambda col: (col == 1).sum() / len(col))\n",
    "    print(prevalence)\n",
    "    # 3. 确定 Rare / Common 标签\n",
    "    rare_threshold = prevalence.quantile(rare_q)\n",
    "    common_threshold = prevalence.quantile(common_q)\n",
    "\n",
    "    print(rare_threshold,common_threshold,sep='\\n')\n",
    "    rare_labels = prevalence[prevalence <= rare_threshold].index.tolist()\n",
    "    common_labels = prevalence[prevalence >= common_threshold].index.tolist()\n",
    "\n",
    "    print(\"Rare labels:\", rare_labels)\n",
    "    print(\"Common labels:\", common_labels)\n",
    "\n",
    "    # 4. 定义函数来判断一个样本属于哪个组\n",
    "    def assign_group(row):\n",
    "        has_rare = any(row[label] == 1 for label in rare_labels)\n",
    "        has_common = any(row[label] == 1 for label in common_labels)\n",
    "\n",
    "        if has_rare and not has_common:\n",
    "            return \"Rare\"\n",
    "        elif has_common and not has_rare:\n",
    "            return \"Common\"\n",
    "        else:\n",
    "            return \"Other\"  # 混合或无标签\n",
    "\n",
    "    df[\"subgroup\"] = df.apply(assign_group, axis=1)\n",
    "\n",
    "    # 5. 保存结果\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"处理完成！结果已保存到 {output_path}\")\n",
    "\n",
    "\n",
    "build_subgroups(\"./your_path\", \"output_with_subgroups.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义Rara-subgroup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rare_labels= ['Consolidation', 'Pneumonia', 'Pneumothorax', 'Pleural Other']\n",
    "common_labels= ['Cardiomegaly', 'Lung Opacity', 'Atelectasis', 'Support Devices']\n",
    "\n",
    "df_test = pd.read_csv('./your_path.csv')\n",
    "df_labeled = pd.read_csv('./your_path.csv')\n",
    "# print(df_test['study_id'].head())\n",
    "# print(df_labeled['study_id'].head())\n",
    "# print(df_labeled['study_id']==df_test['study_id'])\n",
    "# 定义一个函数来判断每行属于哪个亚组\n",
    "def assign_subgroup(row):\n",
    "    rare_flag = any(row[label] == 1 for label in rare_labels)\n",
    "    common_flag = any(row[label] == 1 for label in common_labels)\n",
    "    \n",
    "    if rare_flag and not common_flag:\n",
    "        return 'CF'\n",
    "    elif common_flag and not rare_flag:\n",
    "        return 'CFCL'\n",
    "    elif rare_flag and common_flag:\n",
    "        return 'CFPF'\n",
    "    else:\n",
    "        return 'all'\n",
    "\n",
    "# 添加亚组列\n",
    "df_labeled['Subgroup'] = df_labeled.apply(assign_subgroup, axis=1)\n",
    "\n",
    "# 查看每个亚组的样本数\n",
    "subgroup_counts = df_labeled['Subgroup'].value_counts()\n",
    "print(subgroup_counts)\n",
    "df_test['Subgroup'] = df_labeled['Subgroup']\n",
    "# 如果需要，可以保存到新的 CSV\n",
    "df_test.to_csv(\"./your_path.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XrayRG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
